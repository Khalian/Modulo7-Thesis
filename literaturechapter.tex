\chapter{Literature Review}

\noindent Music Information Retrieval is an active and vibrant discipline. Both academia and industry diligently pursue it albeit with different goals in mind. While academia's primary aim is to explore particular problems (e.g cover song detection \cite{coversongid}, estimating chords from chroma vectors \cite{chord-detection} ) etc, the Industry is primarily interested in solving problems like song recommendation and similarity searches for mass consumption. The following sections outlines the software efforts and research problems tackled by MIR community in general. 

\section{Current MIR Software}
\noindent Both Industry and Academia have created an extensive set of software for solving these problems. The following is an overview of such software used in production and the problems they attempt to address.

\subsection{jMIR}

\noindent jMIR  \cite{jMIR}, or Java Music Information Retrieval tool set is a collection of Java code, GUI, API and CLI tools for the purpose of feature extraction from variety of music sources (in particular audio and midi file formats) and mine cultural information from the web. jMIR extracts an exhaustive set of features that can be used in machine learning tasks. The primary use of jMIR is automatic music classification and feature extraction and not similarity computations per se (which is one of Modulo7's core goals). Moreover jMIR does not scale to myriad sources of music in existence. Unlike Modulo7, jMIR also relies on faithful recordings and does not attempt to fill up missing information (like key signature estimation etc). Nevertheless its one of the best open source MIR software in existence especially for MIR research involving machine learning approaches. 

\subsection{Marsyas}

\noindent Marsyas \cite{marsyas} (Music Analysis, Retrieval and Synthesis for Audio Signals) is a software stack for audio processing with specific emphasis on Music Information Retrieval and music signal extraction. Marsyas is a heavily developed and a widely utilized state of the art framework for audio processing but also has a steep learning curve. Modulo7 has very different goals (multiple format support, music similarity, structured querying etc) as compared to marsyas.

\subsection{SIMILIE}

\noindent SIMILIE \cite{similie} is a set of tools for music similarity measures used for monophonic melodies and features multiple approaches to construct vector space models for melodies. The techniques used for melodic similarity analysis in SIMILIE are novel and derive from many subfields such as Natural Language Processing. Modulo7 uses a subset of these similarity measures as basis for an extended an improved model of similarities based on polyphonic music and harmonic elements. Moreover SIMILIE needs its own file format (called .mcsv) for analysis. Although the software package gives a converter for different sources, its not as variegated as Modulo7's format support is (which directly parses different music source files).

\subsection{Echo Nest APIs}
 
\noindent Echo Nest \cite{echonestfingerprint} is a company that specializes in big data music intelligence. Echo Nest APIs and backend powers many music platforms like last.fm, Spotify etc. In particular Echo Nest provides APIs for extraction of audio features, acquiring artists similar to a particular artist etc. Echo Nest API is used for some sub tasks in Modulo7 described in \ref{mp3format} \\\\
Echo Nest also maintains the worlds biggest music database as well as data mined from them along with extracted audio features, web mined information, user preference etc). 

\subsection{Humdrum}

\noindent Humdrum \cite{humdrum} is a set of tools for computer based automation and assistance in musicology research. Humdrum has the capability for solving very complex questions using music theoretic concepts. It supports its own file format for analysis of music called the kern format \cite{humdrumkern}. Humdrum is specifically designed for musicologists for automating tasks that they otherwise would have required manual analysis but gathering statistics, music classification or music similarity analysis are not end goals for Humdrum. The fundamental difference of Modulo7 over humdrum is Modulo7 acts as a bulk analysis and querying tool while humdrum is designed for specific and in depth analysis of songs.

\subsection{Gamera}

\noindent Gamera \cite{gamera} is Optical Symbol Recognition(OMR) open Source software based on supervised and hybrid learning approaches for training. Gamera is designed with the particular aim of symbol recognition of old documents and is extensible to scriptures and languages. Gamera also supports creating of new plugins for custom tasks and is widely used in academia for OMR. 

\subsection{Audiveris} \label{audiveris}

\noindent Audiveris \cite{audiverishandbook} is an Open source software for Optical Music Recognition. Unlike Gamera, Audiveris can be directly consumed as a service for the purpose of OMR.  Audiveris is used as service in many leading Notation Platforms like Musescore etc. As such, Audiveris is used as a subcomponent of Modulo7's architecture for Optical Music Recognition System. \ref{digitizedsheet}. 

\section{Music Representation Formats} \label{musicrepresentation}

\noindent Modulo7 parses multiple formats for music described in \ref{m7songsources}. However there are many other sources prevalent in academia that are worth mentioning. \\

\noindent \textbf{GUIDO :} GUIDO musical notation format is a computer notation format that is made to logically represent symbolic musical information that is easily readable by both humans and computers and can be stored as a text file. \\

\noindent \textbf{KERN :} The kern format \cite{humdrumkern} is used in humdrum to symbolically denote events in columns while voices are represented in rows. \cite{humdrum}. This facilitates a columnar representation of music on which humdrum can perform  different kinds of music theoretic analysis. 

\section{Typical problems of MIR}

\noindent On top of the generic software created by researchers and industry experts, experts have tackled specific problems in Music Cognition\cite{musiccog}, classification\cite{jMIR}, query by Humming Systems \cite{shazam} etc. Broadly speaking, the problem statement falls in the following broad categories

\subsection{Music Classification / Genre Identification}

\noindent The problem of music classification is to assign a tag (also called a genre of a song) which broadly categorizes it according to some criteria. While the genre definitions for songs are often vague, it helps in giving information about which songs are relevant based on a coarse criteria of what "type" a particular song is. Companies like Pandora and Microsoft assign genres to songs via musicologists \cite{genreclassification} which means highly trained people manually classify music. Such approaches are expensive in terms of human labor and prone to errors. Automatic Music Classification takes a different approach using algorithms and machine learning approaches like jMIR \cite{jMIR} does to classify music. 

\subsection{Music Similarity Analysis}

\noindent The problem of music similarity analysis lies at the heart of a large number other applications like Song Identification, Query by humming systems etc. Most literature have addressed the problem of monophonic melodic similarity \cite{simsurvey} and not on generic polyphonic similarity. There are many systems \cite{similie} and music databases in existence \cite{humdrumkern} for the purpose of music similarity analysis.

\subsection{Automated Musicological Research}

\noindent In many cases musicological research is conducted manually by applying rules and music theoretic criteria. An example would be applying counterpoint analysis techniques given the rules in a treatise \cite{theorytreatise} to music sheet manually. This is labor intensive and the research community tries to address this inefficiency via techniques to automate analysis of music. A significant effort is done by the Humdrum community \cite{humdrum} in automated musicological research. 

\subsection{Audio processing and feature extraction}

\noindent Most music is represented in audio format rather than symbolic format, as consumption of music is primarily for the layman or the musically uninitiated. One task would be music transcription(also known as melody extraction \cite{melextract}) to convert audio to symbolic formats which allows for subsequent symbolic analysis. However researchers have only found success in melody extraction where one voice is clearly dominant in a recording \cite{melextract}. Researchers have also worked on quantitatively defining the concept of timbre (a peculiar tonal quality of a voice independent of pitch and loudness which characterizes the source of the sound) with varying degrees of success both qualitatively \cite{timbrequal} and computationally \cite{musiclisteningthesis}. 

\subsection{Intelligent Music Archiving and Retrieval}

\noindent Key to music information retrieval are efficient and novel techniques to archive musical sources so that meaningful queries can be made against these archived sources. Many libraries and library sciences programs work actively in this regard. Our very own Johns Hopkins University Eisenhower Library has a vast collection of Sheet music on American Popular music called the "Lester Levy Sheet Music Collection" \cite{Levy:Collection:Online} . There are many such collections worldwide. There are many labs and institutions which work towards archiving digitized sheet music, notable among them are the DDMAL lab in McGill University \cite{DDMAL} which works in archiving medieval sheet music in a digitized form as well as perform statistical analysis on it. 

\subsection{Music Recommendation}

\noindent Perhaps the most commercialized application of Music Information Retrieval is the task of music recommendation i.e. intelligent suggestion of songs to a user given his or her preferences and/or past listening history. Music recommendation is an end goal in itself and not a distinct problem compared to the previous problems discussed in this section. In order to facilitate this, various music databases \cite{humdrumkern, msd} and query systems are built and comparisons are based on lyrics genre tags and other properties of music data \cite{musicrecSurvey}. Most approaches have been based on collaborative filtering \cite{amazonreco} based on contextual meta data (information extracted from a community of user's judgments and comments on music) and sparingly from low level audio features extracted from a song \cite{musicrecSurvey}. 

\subsection{Audio Fingerprinting and Song ID}

\noindent A very industry relevant problem statement involves fingerprinting audio files and matching these finger prints to an input(melody or fragment of a song) fingerprint. These systems stress on an exact match as an end goal. Many commercial systems are in existence including companies like Shazam \cite{shazam} which have developed sophisticated algorithms and systems dedicated to solve this problem.



