\chapter{Mathematics of Modulo7}
\label{sec:mir math}

\noindent The following sections describe the mathematical concepts used and implemented in Modulo7.

\section{Preprocessing Steps} \label{sim:preprocess}

\noindent It might be that the input sources require certain preprocessing for any mathematical model to work. The following sub sections describe certain preprocessing operations that can be done in order to prepare input data to be transcribed into a vector space model. 

\subsection{Tonality Alignment} \label{tonalityalignment}

\noindent In order to compare two songs in different keys, the songs must be transposed to one key. This transposition shifts every note by a certain interval (same as the intervalic distance between the keys of the input songs.) This is analogous to correcting a global offset such that similarity measures based on string representations of music can be applied.

\subsection{Voice to Melodic Representation Coversion} \label{voicemelconv} 

\noindent Given a voice, various instants inside the voice can be either single notes (melodic notes) or chords (stacks of notes). Often in order to apply pure melodic techniques a voice, a conversion is required from a generic voice to a melody. In order to do that, every chord in the voice is replaced by the root note of the chord. 

\subsection{Contourization} \label{contourization}

\noindent A contour is a quantitative representation of the direction of motion of a given voice / melody. Contours are clearly defined for melodies as a concept and hence the preprocessing steps of converting generic voices to pure melodies.\ref{voicemelconv}. There are many different representations of contour in literature and Modulo7 implements the following representations of contour. \\

\noindent \textbf{Gross Contour : } Gross Contour only contains the information of whether the successive notes of a melody goes up or down irrespective of the intervalic distance by which notes go down or up. Notes going up are designated with value 1, notes going down by -1 and notes staying on the same pitch with 0. So in essence the gross contour is a vector of 0's, 1's and -1s with length = number of melodic intervals present in the voice.\\

\noindent \textbf{Natural Contour : } The natural contour of a song is similar to the gross contour with a difference that the intervalic distance between subsequent notes are calculated instead of ignored as in gross contour. Define the gradient between succesive notes N1 and N2 as 


\section{Vector Space Models of Music}

\noindent In traditional text based information retrieval retrieval systems, documents are indexed and a vector space representation of documents are created. Typical approaches for counting term frequencies or some weighting scheme like Term Frequency-Inverse Document Frequency Approach (TF-IDF). Analogous to text based IR, Music data can also be expressed as a vector space based on the approach taken. Some of these approaches are taken from the SIMILIE \cite{similietechnicalmanual} but generalized for polyphonic music. Many approaches are novel based on the author's music theoretic studies. 

\subsection{Vector Space Models for Monophonic Music}

\noindent Certain vector space models are with respect to a single voice. This allows vector space models to be represented as arrays with simple methods of computation that can be applied on top of it. First define pitch as a real number/string depending on context such that at given instant time $t_i$ either frequency $p_i$ is being played or its note representation $p_i$ is being played. With this simple definition of pitch and onset time we can define our vector space models as follows\\
 
\noindent \textbf{Pitch Vector:} A voice can be expressed as a sequence of pitches $n_i$ = ($p_i, t_i$) where $p_i$ is the pitch and/or the set of pitches at instant of time $t_i$. The symbolic representation of music essentially a discretized version of these values from music sources and hence a vector representation can be made. A voice V can be represented as a vector

\begin{equation}
P = <n_1, n_2, ... n_n>
\end{equation}

\noindent A similar vector representation could be when the time information is eschewed in favor on only the pitches. This vector is called the raw pitch vector and is denoted as the follows :-

\begin{equation} \label{eq:rawpitch}
R = <p_1, p_2, ..., p_n>
\end{equation}

\noindent \textbf{Pitch Interval Vector:} Another way to look at elements is the interval spacing between elements. This is same as the interval concept in the music theory chapter. Mathematically an interval is defined as $\Delta p_i = p_i - p_{i-1}$. And thus an pitch interval vector is defined as

\begin{equation}
PI = <\Delta p_1, \Delta p_2, ... , \Delta p_n>
\end{equation}

\noindent \textbf{Rhythmically weighted Pitch Interval Vector:} In order to include the rhythmic information in the pitch interval Vector, define rhythmically weighted pitch as $rp_i = \Delta p_i \times t_i$. Now the rhythmically weighted pitch vector can be represented as

\begin{equation}
RPI = <rp_1, rp_2, ... rp_n>
\end{equation}

\begin{equation}
gr(N_1, N2_2) = Interval_{dist}(N_1, N_2)
\end{equation}

Thus then the natural contour can be defined as:-

\begin{equation}
NC(Voice) = (gr(N_0, N_1), gr(N_1, N_2), .... gr(N_{len_{Voice} - 1}, N_{len_{Voice}}))
\end{equation}

where $N_i$ is the $i^{th}$ note in the voice after converting it into a melody \ref{voicemelconv}

\subsection{Vector Space Models for Polyphonic Music} 

\noindent \textbf{Normalized Tonal Histogram Vector:} The tonal histogram is a vector or map of 12 distinct intervals present in western music theory and the number of time. Each position in the vector corresponds to the total number of times that interval has occurred in a song. This is the total summation of the intervals over each individual voice. Mathematically define $\Delta P^{voice_j} = \sum_{i=1}^{len(voice)} p_i^{voice_j}$ and for a song $\Delta P^{song} = \sum_{voice_j} \Delta P^{voice_j}$. Define interval fraction as : $\Delta p^f_i = \frac{sum_i \Delta p_i}{\Delta P^{song}}$ where $p_i$ stands for the interval quantity = i. Thus we can define the normalized tonal histogram vector as

\begin{equation}
NTH = <\Delta p^f_1, \Delta p^f_2, ... , \Delta p^f_{12}>
\end{equation}

\noindent \textbf{Normalized Tonal Duration Histogram Vector:} The tonal duration histogram is a vector or map of 12 distinct intervals present in western music theory. Each position in the vector corresponds to the cumulative duration for which that interval has occurred in a song.This is the total summation of the duration of intervals over each individual voice for the entire song. Mathematically define $\Delta T^{voice_j} = \sum_{i=1}^{len(voice)} t_i^{voice_j}$ and for a song $\Delta T^{song} = \sum_{voice_j} \Delta T^{voice_j}$. Define durational interval fraction as : $\Delta t^f_i = \frac{\sum_i \Delta t_i}{\Delta T^{song}}$ where the sum in the numerator stands for the cumulative duration for which an interval quantity = i is played. Thus we can define the normalized tonal duration histogram vector as

\begin{equation}
NTDH = <\Delta t^f_1, \Delta t^f_2, ... , \Delta t^f_{12}>
\end{equation}

\noindent \textbf{Normalized Pitch Duration Histogram Vector:}  The pitch duration histogram is a vector or map of 12 distinct pitches present in western music theory. Each position in the vector corresponds to the cumulative duration for which that pitch has occurred in a voice and for a song it is the summation of cumulative durations over all the voices. Mathematically define $\Delta T^{voice_j} = \sum_{i=1}^{len(voice)} t_i^{voice_j}$ and $\Delta T^{song} = \sum_{voice_j} \Delta T^{voice_j}$. Define durational interval fraction as : $\Delta t^p_i = \frac{\sum_i \Delta p_i}{\Delta T^{voice}}$. Here the summation in the numerator is the cumulative time for which the pitch in the $i^{th}$ position of the western music chromatic scale is played. Thus we can define the normalized tonal duration histogram vector as

\begin{equation} \label{NPDH}
NPDH = <\Delta t^p_1, \Delta t^p_2, ... , \Delta t^p_{12}>
\end{equation}


\section{Similarity Measures} \label{similarity}

\noindent Similarity is defined in Modulo7 as a function which takes as input two voices or songs and outputs a value between 0 to 1 where 0 stands for least similar and 1 stands for most similar. Similarity measures are a cornerstone of recommendations and many recommender engines are based on rank similarity measures for different criteria. Mathematically :-
\begin{equation}
Sim_{song}(S_1, S_2) \in (0, 1)
\end{equation}
\begin{equation}
Sim_{voice}(V_1, V_2) \in (0, 1)
\end{equation}

\subsection{Similarity Measures for Monophonic Music} \label{monophonicSim}

\noindent Similarity measures are different concepts for monophonic and polyphonic music as it stems from comparing different vector representations. For the following sections assume vectors of equal length. In a further section \ref{sim:unequal} we extend standard similarity measures to vectors of unequal length. \\

\noindent \textbf{Edit Distance on Raw Pitch Vector Representation:} Consider the raw pitch vector in equation \ref{eq:rawpitch}. This vector is essentially a vector of tokens or equivalently a string. Hence standard edit distance algorithms in normal text IR can be applied to it (e.g Leveinstein Distance, Wagnerâ€“Fischer algorithm etc \cite{simtour}).

\subsection{Similarity Measures for Polyphonic Music}

\noindent In order to incorporate vector space models to polyphonic similarity, monophonic measures can be extended in order to accomodate for polyphony. Another approach would be to apply measures.

\noindent {Generic maximal voice similarity} An approach would be to take pairwise voice similarities between two voices of a song, and then representing the max of these pairwise computed similarities. This model is especially useful in cases where comparing a melody against a song which contains a similar melody. Mathematically 
\begin{equation}
GMVS(S_1, S_2, VSim) = arg_{max} (VSim(V_i, V_j)) \ s.t \ V_i \in S_1 \ and \ V_j \in S_2
\end{equation}

\subsection{Similarity of vectors of unequal length} \label{sim:unequal}

\noindent Its almost certain that two voices will never have the same length. Hence its important at this point to ascertain how to map similarity measures to unequal length voices. Moreover, its also important to judge which regions of one melody are maximally similar to which other regions of the other melody (also called as alignment) . Modulo7 takes inspiration from bio informatics domain and uses the smith waterman algorithm modified for voice similarity \cite{smithWatermanBook}. The algorithm is as follows:-

\begin{algorithm}

\label{CHalgorithm}
\begin{algorithmic}[1]
\Procedure{Smith Waterman Voice Similarity(V1, V2, InSim)} {}
\State Define WM = Array[len(V1)][len(V2)]
\For {i in 1 to len(V1)}
\State WM[i][0] = 0
\EndFor
\For {j in 1 to len(V2)}
\State WM[0][j] = 0
\EndFor
\For {i in 1 to len(V1)}
\For {j in 1 to len(V2)}
\State WM[i][j] = max(0, WM[i - 1][j - 1] + InSim(V1(i), V2(j)), WM[i - 1, j] + InSim(V1(i), $\phi$), WM[i, j - 1] + InSim($\phi$,V2(j))
\EndFor
\EndFor
\State return WM[len(V1), WM(len(V2))] / max(len(V1), len(V2)
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Meta Data based similarity}

\noindent All the similarity measures considered so far is based on similarity on voices and sets of voices in a a song. However there are other global properties of a song such as the key signature or the time signature of a song. This global information can give us context about a song's particular characteristics (for example songs in Minor scale are generally sadder than songs in the Major scale). Hence estimates can be more quickly derived by comparing meta data features rather than voices (whose computation). These similarity measures can be used for a additional purposes(for example completing incomplete meta data). 

\subsection{Tonal Similarity}

\noindent Often pieces of one key are similar to pieces on a different key, simply based on the fact that the keys themselves are similar. As a result a similarity measure could be developed which takes into account the differences in the key signature of a song

\begin{equation}
Sim(K_1, K_2) = 1 if (K1 == K2) ; 0 otherwise
\end{equation}


\section{Criteria Analysis} \label{criteria}

\noindent While Modulo7's primary goal is on comparing similarities between pieces, often its better to ascertain whether a certain piece satisfies a certain music theoretic predicate. Some example problems of such sorts are if the piece has a species 1 counterpoint (i.e. the voices move with the exact same speed) or if the piece has voices in the STAB criteria (with exactly 4 voices and their ranges being in particular range of high and low notes). This allows a consumer to build complex queries based on pieces satisfying selectivity requirements on top of similarity measures or alternatively if certain pieces just satisfies a one or more criteria. Following are the criteria implemented in Modulo7

\subsection{Simple criteria} 

\noindent Simple criteria are based on simple global level properties of the song. \\

\noindent \textbf{Polyphonic Criteria:} Its a simple criteria which decides whether a piece of music is polyphonic or not. This is decided on the basis of the number of voices in the song. \\

\noindent \textbf{Key Signature Equality Criteria:} Its a simple criteria that checks if a song is in a particular key or not.

\section{Statistics Analysis} \label{statistic}

\noindent A statistic when applied to a given song outputs a real number. Alternatively statistics could be thought of a non trivial extracted single value features. Mathematically a feature can be defined as:-

\begin{equation}
Criteria(Song) = x \ s.t \ x \in \mathbb{R}
\end{equation}

\noindent The following are the statistics implemented in Modulo7. \\

\noindent \textbf{Melodic Repeatability Fraction: } Given a voice, compute a sub voice that repeats the maximum number of times within the voice and then take the fraction between the length sub voice which satisfies this criteria against the length of the voice. This measure also uses the pre-processing step \\

\noindent \textbf{Interval Index: } \label{intervals} An interval index is the fraction of intervals being played in a song divided by the total number of intervals present in the song. These statistics are coarse measures of a song. There are three classes of interval indices:-

\begin{enumerate}
\item Happiness Index : The happiness index of a song is the number of major intervals in a song divided by the total number of intervals. A major interval sounds "happy" to a layman hence a higher concentration of them makes a song happier \cite{majorvsminorintervals}. 
\item Sadness Index : The sadness index of a song is the number of minor intervals \cite{minorintervalssad}  in a song divided by the total number of intervals. A minor interval sounds "sad" to a layman hence a higher concentration of them makes a song sadder \cite{majorvsminorintervals}. 
\item Power Index : The power index of a song is the number of perfect interval in a song divided by the total number of intervals. Perfect melodic intervals are very prevalent in a rock and metal songs and are an expression of a neutral/powerful tone. This stems from the fact that perfect fifths along with perfect unison or perfect octaves, which are very common in rock music \cite{foundationsOfRock} 
\end{enumerate}